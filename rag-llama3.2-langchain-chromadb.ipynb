{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n",
    "\n",
    "## Objective\n",
    "\n",
    "Use Llama 2.0, Langchain and ChromaDB to create a Retrieval Augmented Generation (RAG) system. This will allow us to ask questions about our documents (that were not included in the training data), without fine-tunning the Large Language Model (LLM).\n",
    "When using RAG, if you are given a question, you first do a retrieval step to fetch any relevant documents from a special database, a vector database where these documents were indexed. \n",
    "\n",
    "## Definitions\n",
    "\n",
    "* LLM - Large Language Model  \n",
    "* Llama 2.0 - LLM from Meta \n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
    "* ChromaDB - vector database  \n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "## Model details\n",
    "\n",
    "* **Model**: Llama 2  \n",
    "* **Variation**: 7b-chat-hf  (7b: 7B dimm. hf: HuggingFace build)\n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n",
    "\n",
    "LlaMA 2 model is pretrained and fine-tuned with 2 Trillion tokens and 7 to 70 Billion parameters which makes it one of the powerful open source models. It is a highly improvement over LlaMA 1 model.\n",
    "\n",
    "\n",
    "## What is a Retrieval Augmented Generation (RAG) system?\n",
    "\n",
    "Large Language Models (LLMs) has proven their ability to understand context and provide accurate answers to various NLP tasks, including summarization, Q&A, when prompted. While being able to provide very good answers to questions about information that they were trained with, they tend to hallucinate when the topic is about information that they do \"not know\", i.e. was not included in their training data. Retrieval Augmented Generation combines external resources with LLMs. The main two components of a RAG are therefore a retriever and a generator.  \n",
    " \n",
    "The retriever part can be described as a system that is able to encode our data so that can be easily retrieved the relevant parts of it upon queriying it. The encoding is done using text embeddings, i.e. a model trained to create a vector representation of the information. The best option for implementing a retriever is a vector database. As vector database, there are multiple options, both open source or commercial products. Few examples are ChromaDB, Mevius, FAISS, Pinecone, Weaviate. Our option in this Notebook will be a local instance of ChromaDB (persistent).\n",
    "\n",
    "For the generator part, the obvious option is a LLM. In this Notebook we will use a quantized LLaMA v2 model, from the Kaggle Models collection.  \n",
    "\n",
    "The orchestration of the retriever and generator will be done using Langchain. A specialized function from Langchain allows us to create the receiver-generator in one line of code.\n",
    "\n",
    "## More about this  \n",
    "\n",
    "Do you want to learn more? Look into the `References` section for blog posts and in `More work on the same topic` for Notebooks about the technologies used here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations, imports, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:26:49.602554Z",
     "iopub.status.busy": "2025-03-20T13:26:49.602263Z",
     "iopub.status.idle": "2025-03-20T13:29:20.393034Z",
     "shell.execute_reply": "2025-03-20T13:29:20.391745Z",
     "shell.execute_reply.started": "2025-03-20T13:26:49.602525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.33.0\n",
      "  Using cached transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting accelerate==0.22.0\n",
      "  Using cached accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting einops==0.6.1\n",
      "  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain==0.0.300\n",
      "  Using cached langchain-0.0.300-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting xformers==0.0.21\n",
      "  Using cached xformers-0.0.21.tar.gz (22.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes==0.41.1\n",
      "  Using cached bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting sentence_transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting chromadb==0.4.12\n",
      "  Using cached chromadb-0.4.12-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from accelerate==0.22.0) (2.6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (3.11.13)\n",
      "Collecting anyio<4.0 (from langchain==0.0.300)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (1.33)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.300)\n",
      "  Using cached numexpr-2.10.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (2.10.6)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.300)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.15.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.2.0)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.300)\n",
      "  Using cached pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl.metadata (153 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n",
      "  Using cached chroma_hnswlib-0.7.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.12)\n",
      "  Using cached fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (4.12.2)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n",
      "  Using cached pulsar_client-3.6.1-cp310-cp310-macosx_13_0_universal2.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (1.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (6.5.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (0.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (3.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.9.0.post0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2.3.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.12) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (13.9.4)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from nltk->sentence_transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (11.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (0.1.2)\n",
      "Using cached transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Using cached langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
      "Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "Using cached chromadb-0.4.12-py3-none-any.whl (426 kB)\n",
      "Using cached chroma_hnswlib-0.7.3-cp310-cp310-macosx_11_0_arm64.whl (197 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
      "Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Using cached numexpr-2.10.2-cp310-cp310-macosx_11_0_arm64.whl (134 kB)\n",
      "Using cached pulsar_client-3.6.1-cp310-cp310-macosx_13_0_universal2.whl (8.0 MB)\n",
      "Using cached pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[214 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/version.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/sum_strided.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_sum.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/reversible.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nvfuser.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_stride_sum.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_encoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_pytorch_transformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_factory.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/model_factory.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_configs.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/weight_init.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/local.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/random.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/visual.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/favor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_dropout_res_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_act_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_dropout_res.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/indexing\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m clang++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/anaconda3/envs/ai_env/include -arch arm64 -fPIC -O2 -isystem /opt/anaconda3/envs/ai_env/include -arch arm64 -I/private/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/pip-install-ha9ni2m3/xformers_fde43e251d304c219c935d1a3225eb27/xformers/csrc -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/TH -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/THC -I/opt/anaconda3/envs/ai_env/include/python3.10 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/attention.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_clang\\\" -DPYBIND11_STDLIB=\\\"_libcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1002\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m clang++: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang++' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 langchain==0.0.300 xformers==0.0.21 \\\n",
    "bitsandbytes==0.41.1 sentence_transformers==2.2.2 chromadb==0.4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:20.396669Z",
     "iopub.status.busy": "2025-03-20T13:29:20.396080Z",
     "iopub.status.idle": "2025-03-20T13:29:26.899824Z",
     "shell.execute_reply": "2025-03-20T13:29:26.899012Z",
     "shell.execute_reply.started": "2025-03-20T13:29:20.396640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import os\n",
    "import transformers\n",
    "import time\n",
    "#import chromadb\n",
    "#from chromadb.config import Settings\n",
    "\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.document_loaders import TextLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import (\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, tokenizer, query pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, the device, and the `bitsandbytes` configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:26.901324Z",
     "iopub.status.busy": "2025-03-20T13:29:26.900871Z",
     "iopub.status.idle": "2025-03-20T13:29:26.955062Z",
     "shell.execute_reply": "2025-03-20T13:29:26.954421Z",
     "shell.execute_reply.started": "2025-03-20T13:29:26.901289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device => mps\n"
     ]
    }
   ],
   "source": [
    "# Set the device\n",
    "\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "print(f\"Using device => {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:26.957030Z",
     "iopub.status.busy": "2025-03-20T13:29:26.956689Z",
     "iopub.status.idle": "2025-03-20T13:31:44.637120Z",
     "shell.execute_reply": "2025-03-20T13:31:44.636242Z",
     "shell.execute_reply.started": "2025-03-20T13:29:26.956999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device => mps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain AI in one sentence: Artificial intelligence (AI) refers to a computer system that can perform tasks that would typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
      "\n",
      "Explain the role of Machine Learning (ML) in AI: Machine learning (ML)\n"
     ]
    }
   ],
   "source": [
    "# 選擇要使用的 LLaMA 模型（例如 \"meta-llama/Llama-3.2-3B-Instruct\"）\n",
    "hf_token = os.environ.get(\"HF_TOKEN\") \n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# 檢查裝置\n",
    "device = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "print(\"Using device =>\", device)\n",
    "\n",
    "# 載入 config\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "# 載入 model\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# 載入 tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# 簡易測試\n",
    "prompt = \"Explain AI in one sentence:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the query pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:44.638973Z",
     "iopub.status.busy": "2025-03-20T13:31:44.638336Z",
     "iopub.status.idle": "2025-03-20T13:31:46.271505Z",
     "shell.execute_reply": "2025-03-20T13:31:46.270705Z",
     "shell.execute_reply.started": "2025-03-20T13:31:44.638940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.049 sec.\n"
     ]
    }
   ],
   "source": [
    "# 建立 text-generation pipeline\n",
    "time_1 = time.time()\n",
    "\n",
    "query_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
    "    device_map=\"auto\",  # 或直接指定 device=device\n",
    "    max_length=1024\n",
    ")\n",
    "\n",
    "time_2 = time.time()\n",
    "print(f\"Prepare pipeline: {round(time_2 - time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:46.272769Z",
     "iopub.status.busy": "2025-03-20T13:31:46.272516Z",
     "iopub.status.idle": "2025-03-20T13:31:46.277701Z",
     "shell.execute_reply": "2025-03-20T13:31:46.276793Z",
     "shell.execute_reply.started": "2025-03-20T13:31:46.272746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    \"\"\"\n",
    "    Perform a query and print the result.\n",
    "    Args:\n",
    "        tokenizer: the tokenizer\n",
    "        pipeline: the text-generation pipeline\n",
    "        prompt_to_test: the prompt (string)\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    time_1 = time.time()\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,\n",
    "        top_k=10,\n",
    "        num_return_sequences=1,\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200,\n",
    "    )\n",
    "    time_2 = time.time()\n",
    "    print(f\"Test inference: {round(time_2 - time_1, 3)} sec.\")\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the query pipeline\n",
    "\n",
    "We test the pipeline with a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:46.281510Z",
     "iopub.status.busy": "2025-03-20T13:31:46.281274Z",
     "iopub.status.idle": "2025-03-20T13:31:51.993780Z",
     "shell.execute_reply": "2025-03-20T13:31:51.992809Z",
     "shell.execute_reply.started": "2025-03-20T13:31:46.281490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 4.739 sec.\n",
      "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. The State of the Union address is a formal address delivered by the President of the United States to Congress and the public, where the President reports on the state of the country, the economy, and the nation's progress. The address is typically held in the evening of the first Monday in January, and it provides an overview of the President's agenda, accomplishments, and challenges for the upcoming year. The speech is usually around 45 minutes long.\n"
     ]
    }
   ],
   "source": [
    "# 測試一下模型\n",
    "test_model(\n",
    "    tokenizer,\n",
    "    query_pipeline,\n",
    "    \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
     "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
     "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
     "shell.execute_reply": "2023-09-23T19:22:16.439217Z",
     "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z"
    }
   },
   "source": [
    "## Check the model with a HuggingFace pipeline\n",
    "\n",
    "\n",
    "We check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:51.995012Z",
     "iopub.status.busy": "2025-03-20T13:31:51.994748Z",
     "iopub.status.idle": "2025-03-20T13:31:56.181771Z",
     "shell.execute_reply": "2025-03-20T13:31:56.180949Z",
     "shell.execute_reply.started": "2025-03-20T13:31:51.994988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_85431/782496977.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_85431/782496977.py:5: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. The State of the Union address is a formal speech given by the President of the United States to Congress, in which the President reports on the state of the nation, discusses current issues, and outlines legislative proposals. It is a significant event in American politics and is broadcast live on television.\n"
     ]
    }
   ],
   "source": [
    "# 將 pipeline 包裝成 HuggingFacePipeline，方便在 LangChain 中使用\n",
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "# 簡單測試 llm\n",
    "response = llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")\n",
    "print(\"LLM response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of data using Text loder\n",
    "\n",
    "We will ingest the newest presidential address, from Jan 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.183114Z",
     "iopub.status.busy": "2025-03-20T13:31:56.182807Z",
     "iopub.status.idle": "2025-03-20T13:31:56.199371Z",
     "shell.execute_reply": "2025-03-20T13:31:56.198742Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.183090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n"
     ]
    }
   ],
   "source": [
    "# 載入文本\n",
    "loader = TextLoader(\"biden-sotu-2023-planned-official.txt\",\n",
    "                    encoding=\"utf8\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in chunks\n",
    "\n",
    "We split data in chunks using a recursive character text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.200813Z",
     "iopub.status.busy": "2025-03-20T13:31:56.200476Z",
     "iopub.status.idle": "2025-03-20T13:31:56.217027Z",
     "shell.execute_reply": "2025-03-20T13:31:56.216199Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.200783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total splits: 43\n"
     ]
    }
   ],
   "source": [
    "# 切分文本\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=20\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "print(f\"Total splits: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings and Storing in Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (3.4.1)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (4.49.0)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (4.67.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (2.6.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (1.15.2)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (0.29.3)\n",
      "Requirement already satisfied: Pillow in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence-transformers) (11.0.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2024.6.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (4.12.2)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (1.26.4)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.0)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.5.3)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (2.1.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->huggingface-hub>=0.20.0->sentence-transformers) (2025.1.31)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install sentence-transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.218365Z",
     "iopub.status.busy": "2025-03-20T13:31:56.218064Z",
     "iopub.status.idle": "2025-03-20T13:32:35.967071Z",
     "shell.execute_reply": "2025-03-20T13:32:35.966381Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.218335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_85431/1490582424.py:5: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"token\":hf_token})\n"
     ]
    }
   ],
   "source": [
    "# 建立 Embeddings (sentence-transformers/all-mpnet-base-v2)\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": device}  # 指定 device\n",
    "\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"token\":hf_token})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chromadb in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (0.6.3)\n",
      "Requirement already satisfied: build>=1.0.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.2.2.post1)\n",
      "Requirement already satisfied: pydantic>=1.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (2.10.6)\n",
      "Requirement already satisfied: chroma-hnswlib==0.7.6 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.7.6)\n",
      "Requirement already satisfied: fastapi>=0.95.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.115.11)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.34.0)\n",
      "Requirement already satisfied: numpy>=1.22.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.26.4)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (3.21.0)\n",
      "Requirement already satisfied: typing_extensions>=4.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (4.12.2)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.21.0)\n",
      "Requirement already satisfied: opentelemetry-api>=1.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-grpc>=1.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-fastapi>=0.41b0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-sdk>=1.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.31.1)\n",
      "Requirement already satisfied: tokenizers>=0.13.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.48.9)\n",
      "Requirement already satisfied: tqdm>=4.65.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (4.67.1)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (6.5.2)\n",
      "Requirement already satisfied: grpcio>=1.58.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (1.71.0)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.15.2)\n",
      "Requirement already satisfied: kubernetes>=28.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (32.0.1)\n",
      "Requirement already satisfied: tenacity>=8.2.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (9.0.0)\n",
      "Requirement already satisfied: PyYAML>=6.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (6.0.2)\n",
      "Requirement already satisfied: mmh3>=4.0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (5.1.0)\n",
      "Requirement already satisfied: orjson>=3.9.12 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (3.10.15)\n",
      "Requirement already satisfied: httpx>=0.27.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (0.27.0)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb) (13.9.4)\n",
      "Requirement already satisfied: packaging>=19.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (24.2)\n",
      "Requirement already satisfied: pyproject_hooks in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (1.2.0)\n",
      "Requirement already satisfied: tomli>=1.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from build>=1.0.3->chromadb) (2.0.1)\n",
      "Requirement already satisfied: starlette<0.47.0,>=0.40.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from fastapi>=0.95.2->chromadb) (0.46.1)\n",
      "Requirement already satisfied: anyio in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (4.6.2)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.0.2)\n",
      "Requirement already satisfied: idna in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (3.10)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpx>=0.27.0->chromadb) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.27.0->chromadb) (0.14.0)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.9.0.post0)\n",
      "Requirement already satisfied: google-auth>=1.0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.38.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.32.3)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.0.0)\n",
      "Requirement already satisfied: oauthlib>=3.2.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (3.2.2)\n",
      "Requirement already satisfied: urllib3>=1.24.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (2.3.0)\n",
      "Requirement already satisfied: durationpy>=0.7 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from kubernetes>=28.1.0->chromadb) (0.9)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb) (1.13.1)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-api>=1.2.0->chromadb) (8.6.1)\n",
      "Requirement already satisfied: googleapis-common-protos~=1.52 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.69.2)\n",
      "Requirement already satisfied: opentelemetry-exporter-otlp-proto-common==1.31.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-proto==1.31.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-exporter-otlp-proto-grpc>=1.2.0->chromadb) (1.31.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-asgi==0.52b1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.52b1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.52b1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: opentelemetry-util-http==0.52b1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (0.52b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (1.17.2)\n",
      "Requirement already satisfied: asgiref~=3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from opentelemetry-instrumentation-asgi==0.52b1->opentelemetry-instrumentation-fastapi>=0.41b0->chromadb) (3.8.1)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (2.2.1)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb) (1.9.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from pydantic>=1.9->chromadb) (2.27.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->chromadb) (2.15.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from tokenizers>=0.13.2->chromadb) (0.29.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb) (1.5.4)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb) (15.0.1)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (5.5.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (4.9)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers>=0.13.2->chromadb) (2024.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api>=1.2.0->chromadb) (3.21.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->chromadb) (0.1.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->kubernetes>=28.1.0->chromadb) (3.3.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio->httpx>=0.27.0->chromadb) (1.2.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb) (10.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb) (1.3.0)\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth>=1.0.1->kubernetes>=28.1.0->chromadb) (0.6.1)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install chromadb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:35.968365Z",
     "iopub.status.busy": "2025-03-20T13:32:35.968113Z",
     "iopub.status.idle": "2025-03-20T13:32:38.687578Z",
     "shell.execute_reply": "2025-03-20T13:32:38.686690Z",
     "shell.execute_reply.started": "2025-03-20T13:32:35.968343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# 建立向量資料庫 (Chroma)\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=\"chroma_db\"\n",
    ")\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
    "If the answer is not in the context, say \"No relevant info found.\"\n",
    "\n",
    "Return only the final answer in one to three sentences.\n",
    "Do not restate the question or context. \n",
    "Do not include these instructions in your final output.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.689063Z",
     "iopub.status.busy": "2025-03-20T13:32:38.688802Z",
     "iopub.status.idle": "2025-03-20T13:32:38.693716Z",
     "shell.execute_reply": "2025-03-20T13:32:38.692919Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.689041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                # 你前面包裝好的 HuggingFacePipeline\n",
    "    chain_type=\"stuff\",     # 原本就是 \"stuff\" 或其他 chain_type\n",
    "    retriever=retriever,\n",
    "    verbose=False,          # 關閉中間輸出\n",
    "    return_source_documents=False,  # 不回傳檢索文本\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": custom_prompt    # 使用剛才自訂的 prompt\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Retrieval-Augmented Generation \n",
    "\n",
    "\n",
    "We define a test function, that will run the query and time it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.695240Z",
     "iopub.status.busy": "2025-03-20T13:32:38.694954Z",
     "iopub.status.idle": "2025-03-20T13:32:38.707219Z",
     "shell.execute_reply": "2025-03-20T13:32:38.706136Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.695219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def test_rag(qa, query):\n",
    "    \"\"\"只印最終答案，不顯示其他中間資訊\"\"\"\n",
    "    start_time = time.time()\n",
    "    print(f\"Query: {query}\\n\")\n",
    "    result = qa.run(query)\n",
    "    print(\"Final Output:\", result)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    # 你若想顯示 query，可自行 print(query)\n",
    "    # 你若想顯示推理時間，可自行 print\n",
    "    \n",
    "  \n",
    "    # e.g. print(f\"Inference took {end_time - start_time:.2f} sec. Answer: {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check few queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.708562Z",
     "iopub.status.busy": "2025-03-20T13:32:38.708266Z",
     "iopub.status.idle": "2025-03-20T13:32:49.660554Z",
     "shell.execute_reply": "2025-03-20T13:32:49.659722Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.708536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_85431/3684179063.py:5: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
      "If the answer is not in the context, say \"No relevant info found.\"\n",
      "\n",
      "Return only the final answer in one to three sentences.\n",
      "Do not restate the question or context. \n",
      "Do not include these instructions in your final output.\n",
      "\n",
      "Context:\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "Question: What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\n",
      "\n",
      "Answer:\n",
      "The State of the Union report in 2023 was primarily focused on the state of the nation, the economy, and the challenges facing the country. The President highlighted the progress made in addressing key issues such as inflation, healthcare, and education. However, the report also acknowledged the ongoing challenges, including the ongoing conflict in Ukraine and the need to address the growing national debt. The President emphasized the importance of unity and cooperation, and encouraged Americans to come together to address\n"
     ]
    }
   ],
   "source": [
    "# 測試 RAG\n",
    "query = \"What were the main topics in the State of the Union in 2023? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:49.662268Z",
     "iopub.status.busy": "2025-03-20T13:32:49.661709Z",
     "iopub.status.idle": "2025-03-20T13:32:59.736440Z",
     "shell.execute_reply": "2025-03-20T13:32:59.735585Z",
     "shell.execute_reply.started": "2025-03-20T13:32:49.662235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "Final Output: You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
      "If the answer is not in the context, say \"No relevant info found.\"\n",
      "\n",
      "Return only the final answer in one to three sentences.\n",
      "Do not restate the question or context. \n",
      "Do not include these instructions in your final output.\n",
      "\n",
      "Context:\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "Question: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "Answer:\n",
      "The nation's economic status is strong. The State of the Union report indicates a strong economy, with a robust job market, low unemployment, and a growing GDP. The report also highlights the nation's resilience and adaptability in the face of challenges, such as the COVID-19 pandemic. Overall, the report suggests that the nation is in a good place, with a strong foundation for continued growth and prosperity.\n"
     ]
    }
   ],
   "source": [
    "# 再測試另一個 query\n",
    "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document sources\n",
    "\n",
    "Let's check the documents sources, for the last query run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:59.737929Z",
     "iopub.status.busy": "2025-03-20T13:32:59.737646Z",
     "iopub.status.idle": "2025-03-20T13:32:59.775667Z",
     "shell.execute_reply": "2025-03-20T13:32:59.774896Z",
     "shell.execute_reply.started": "2025-03-20T13:32:59.737907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "Retrieved documents: 4\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 查看相似檢索的文件\n",
    "docs = vectordb.similarity_search(query)\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])\n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "\n",
      "To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7860/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Section 17: Create a Gradio Interface (only final answer) ===\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "# 1) Turn off verbose so it won't show chain debug info\n",
    "qa.verbose = False  # This ensures only the final answer is displayed\n",
    "\n",
    "def rag_qa(user_query):\n",
    "    \"\"\"\n",
    "    A simple function that calls qa.run(query) and returns only the final answer.\n",
    "    \"\"\"\n",
    "    return qa.run(user_query)\n",
    "\n",
    "# 2) Provide an English description that clarifies:\n",
    "#    - The document used (Biden's 2023 State of the Union)\n",
    "#    - Some example questions to try\n",
    "\n",
    "demo_description = \"\"\"\n",
    "**Context**:\n",
    "This demo is powered by a Retrieval-Augmented Generation (RAG) approach using \n",
    "Biden’s 2023 State of the Union Address as the primary document. \n",
    "All answers are derived from that transcript. \n",
    "If the answer is not in the text, the system should respond with \"No relevant info found.\"\n",
    "\n",
    "**Sample Questions**:\n",
    "1. What were the main topics regarding infrastructure in this speech?\n",
    "2. How does the speech address the competition with China?\n",
    "3. What does Biden say about job growth in the past two years?\n",
    "4. Does the speech mention anything about Social Security or Medicare?\n",
    "5. What does the speech propose regarding Big Tech or online privacy?\n",
    "\n",
    "Feel free to ask any question relevant to Biden’s 2023 State of the Union Address.\n",
    "\"\"\"\n",
    "\n",
    "# 3) Create a Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_qa,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Biden 2023 SOTU RAG QA Demo\",\n",
    "    description=demo_description\n",
    ")\n",
    "\n",
    "# 4) Launch the Gradio app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "We used Langchain, ChromaDB and Llama 2 as a LLM to build a Retrieval Augmented Generation solution. For testing, we were using the latest State of the Union address from Jan 2023.\n",
    "\n",
    "\n",
    "# More work on the same topic\n",
    "\n",
    "You can find more details about how to use a LLM with Kaggle. Few interesting topics are treated in:  \n",
    "\n",
    "* https://www.kaggle.com/code/gpreda/test-llama-2-quantized-with-llama-cpp (quantizing LLama 2 model using llama.cpp)\n",
    "* https://www.kaggle.com/code/gpreda/fast-test-of-llama-v2-pre-quantized-with-llama-cpp  (quantized Llamam 2 model using llama.cpp)  \n",
    "* https://www.kaggle.com/code/gpreda/test-of-llama-2-quantized-with-llama-cpp-on-cpu (quantized model using llama.cpp - running on CPU)  \n",
    "* https://www.kaggle.com/code/gpreda/explore-enron-emails-with-langchain-and-llama-v2 (Explore Enron Emails with Langchain and Llama v2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References  \n",
    "\n",
    "[1] Murtuza Kazmi, Using LLaMA 2.0, FAISS and LangChain for Question-Answering on Your Own Data, https://medium.com/@murtuza753/using-llama-2-0-faiss-and-langchain-for-question-answering-on-your-own-data-682241488476  \n",
    "\n",
    "[2] Patrick Lewis, Ethan Perez, et. al., Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks, https://browse.arxiv.org/pdf/2005.11401.pdf \n",
    "\n",
    "[3] Minhajul Hoque, Retrieval Augmented Generation: Grounding AI Responses in Factual Data, https://medium.com/@minh.hoque/retrieval-augmented-generation-grounding-ai-responses-in-factual-data-b7855c059322  \n",
    "\n",
    "[4] Fangrui Liu\t, Discover the Performance Gain with Retrieval Augmented Generation, https://thenewstack.io/discover-the-performance-gain-with-retrieval-augmented-generation/\n",
    "\n",
    "[5] Andrew, How to use Retrieval-Augmented Generation (RAG) with Llama 2, https://agi-sphere.com/retrieval-augmented-generation-llama2/   \n",
    "\n",
    "[6] Yogendra Sisodia, Retrieval Augmented Generation Using Llama2 And Falcon, https://medium.com/@scholarly360/retrieval-augmented-generation-using-llama2-and-falcon-ed26c7b14670   \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2880535,
     "sourceId": 4966565,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
