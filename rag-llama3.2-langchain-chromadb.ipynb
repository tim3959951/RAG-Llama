{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "<img src=\"https://www.googleapis.com/download/storage/v1/b/kaggle-forum-message-attachments/o/inbox%2F769452%2Fb18d0513200d426e556b2b7b7c825981%2FRAG.png?generation=1695504022336680&alt=media\"></img>\n",
    "\n",
    "## Objective\n",
    "\n",
    "Build a Retrieval-Augmented Generation (RAG) system using Llama 3.2, Langchain, and ChromaDB. This enables us to query documents not seen during the model’s training, without needing to fine-tune the Large Language Model (LLM).\n",
    "In a RAG setup, a question triggers a retrieval step that fetches relevant documents from a vector database, where the documents have been previously indexed. \n",
    "\n",
    "## Definitions\n",
    "\n",
    "* LLM - Large Language Model  \n",
    "* Llama 3.2 - LLM from Meta \n",
    "* Langchain - a framework designed to simplify the creation of applications using LLMs\n",
    "* Vector database - a database that organizes data through high-dimmensional vectors  \n",
    "* ChromaDB - vector database  \n",
    "* RAG - Retrieval Augmented Generation (see below more details about RAGs)\n",
    "\n",
    "## Model details\n",
    "\n",
    "* **Model**: Llama 3.2  \n",
    "* **Variation**: Llama-3.2-1B-Instruct  (1B: 1B dimm. Llama-3.2: Meta Instruct Build)  \n",
    "* **Version**: V1  \n",
    "* **Framework**: PyTorch  \n",
    "\n",
    "Llama-3.2-1B-Instruct is a multilingual, instruction-tuned model with 1 billion parameters, built for efficiency and ease of use across different languages and tasks. It’s pretrained on a large multilingual dataset and fine-tuned to follow user instructions accurately. This makes it ideal for tasks like conversation, summarisation, and question answering using retrieval. Despite its smaller size, it benefits from the latest LLaMA 3 architecture and is well-suited for lightweight, real-time AI use.\n",
    "\n",
    "\n",
    "## What is a Retrieval Augmented Generation (RAG) system?\n",
    "\n",
    "Large Language Models (LLMs) have shown strong capability in understanding context and delivering accurate responses across various NLP tasks, such as summarisation and question answering. However, while they perform well on information seen during training, they may produce inaccurate responses—or hallucinate—when asked about topics outside their training data. Retrieval-Augmented Generation (RAG) addresses this by combining external sources with LLMs. A typical RAG system includes two core components: a retriever and a generator.  \n",
    " \n",
    "The retriever is responsible for encoding data so that relevant parts can be easily retrieved when queried. This is achieved using text embeddings—vector representations generated by a model trained for this purpose. The most effective way to implement a retriever is through a vector database. There are various options available, both open-source and commercial, such as ChromaDB, Mevius, FAISS, Pinecone, and Weaviate. In this notebook, we will use a local persistent instance of ChromaDB.\n",
    "\n",
    "For the generator, a Large Language Model (LLM) is the natural choice. This notebook uses a quantised LLaMA model.\n",
    "\n",
    "The interaction between the retriever and generator is managed using Langchain. A built-in Langchain function enables us to combine both components with a single line of code.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Installations, imports, utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:26:49.602554Z",
     "iopub.status.busy": "2025-03-20T13:26:49.602263Z",
     "iopub.status.idle": "2025-03-20T13:29:20.393034Z",
     "shell.execute_reply": "2025-03-20T13:29:20.391745Z",
     "shell.execute_reply.started": "2025-03-20T13:26:49.602525Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.33.0\n",
      "  Using cached transformers-4.33.0-py3-none-any.whl.metadata (119 kB)\n",
      "Collecting accelerate==0.22.0\n",
      "  Using cached accelerate-0.22.0-py3-none-any.whl.metadata (17 kB)\n",
      "Collecting einops==0.6.1\n",
      "  Using cached einops-0.6.1-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langchain==0.0.300\n",
      "  Using cached langchain-0.0.300-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting xformers==0.0.21\n",
      "  Using cached xformers-0.0.21.tar.gz (22.3 MB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting bitsandbytes==0.41.1\n",
      "  Using cached bitsandbytes-0.41.1-py3-none-any.whl.metadata (9.8 kB)\n",
      "Collecting sentence_transformers==2.2.2\n",
      "  Using cached sentence_transformers-2.2.2-py3-none-any.whl\n",
      "Collecting chromadb==0.4.12\n",
      "  Using cached chromadb-0.4.12-py3-none-any.whl.metadata (7.0 kB)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.15.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (0.29.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (2024.11.6)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (2.32.3)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers==4.33.0)\n",
      "  Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.3.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (0.5.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from transformers==4.33.0) (4.67.1)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from accelerate==0.22.0) (5.9.0)\n",
      "Requirement already satisfied: torch>=1.10.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from accelerate==0.22.0) (2.6.0)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (2.0.39)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (3.11.13)\n",
      "Collecting anyio<4.0 (from langchain==0.0.300)\n",
      "  Using cached anyio-3.7.1-py3-none-any.whl.metadata (4.7 kB)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (4.0.3)\n",
      "Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (0.6.7)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (1.33)\n",
      "Collecting langsmith<0.1.0,>=0.0.38 (from langchain==0.0.300)\n",
      "  Using cached langsmith-0.0.92-py3-none-any.whl.metadata (9.9 kB)\n",
      "Collecting numexpr<3.0.0,>=2.8.4 (from langchain==0.0.300)\n",
      "  Using cached numexpr-2.10.2-cp310-cp310-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from langchain==0.0.300) (2.10.6)\n",
      "Collecting tenacity<9.0.0,>=8.1.0 (from langchain==0.0.300)\n",
      "  Using cached tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: torchvision in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.21.0)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.6.1)\n",
      "Requirement already satisfied: scipy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (1.15.2)\n",
      "Requirement already satisfied: nltk in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (3.8.1)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sentence_transformers==2.2.2) (0.2.0)\n",
      "Collecting pydantic<3,>=1 (from langchain==0.0.300)\n",
      "  Using cached pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl.metadata (153 kB)\n",
      "Collecting chroma-hnswlib==0.7.3 (from chromadb==0.4.12)\n",
      "  Using cached chroma_hnswlib-0.7.3-cp310-cp310-macosx_11_0_arm64.whl.metadata (252 bytes)\n",
      "Collecting fastapi<0.100.0,>=0.95.2 (from chromadb==0.4.12)\n",
      "  Using cached fastapi-0.99.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: uvicorn>=0.18.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.34.0)\n",
      "Requirement already satisfied: posthog>=2.4.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (3.21.0)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (4.12.2)\n",
      "Collecting pulsar-client>=3.1.0 (from chromadb==0.4.12)\n",
      "  Using cached pulsar_client-3.6.1-cp310-cp310-macosx_13_0_universal2.whl.metadata (1.0 kB)\n",
      "Requirement already satisfied: onnxruntime>=1.14.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (1.21.0)\n",
      "Requirement already satisfied: pypika>=0.48.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (0.48.9)\n",
      "Requirement already satisfied: overrides>=7.3.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (7.4.0)\n",
      "Requirement already satisfied: importlib-resources in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (6.5.2)\n",
      "Requirement already satisfied: bcrypt>=4.0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (4.3.0)\n",
      "Requirement already satisfied: typer>=0.9.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from chromadb==0.4.12) (0.15.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (0.3.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain==0.0.300) (1.18.3)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (3.10)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from anyio<4.0->langchain==0.0.300) (1.2.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (3.26.1)\n",
      "Requirement already satisfied: typing-inspect<1,>=0.4.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (0.9.0)\n",
      "Collecting starlette<0.28.0,>=0.27.0 (from fastapi<0.100.0,>=0.95.2->chromadb==0.4.12)\n",
      "  Using cached starlette-0.27.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.15.1->transformers==4.33.0) (2024.6.1)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain==0.0.300) (3.0.0)\n",
      "Requirement already satisfied: coloredlogs in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: flatbuffers in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (25.2.10)\n",
      "Requirement already satisfied: protobuf in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (5.29.4)\n",
      "Requirement already satisfied: sympy in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from onnxruntime>=1.14.1->chromadb==0.4.12) (1.13.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.16.0)\n",
      "Requirement already satisfied: monotonic>=1.5 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.6)\n",
      "Requirement already satisfied: backoff>=1.10.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.2.1)\n",
      "Requirement already satisfied: python-dateutil>2.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (2.9.0.post0)\n",
      "Requirement already satisfied: distro>=1.5.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from posthog>=2.4.0->chromadb==0.4.12) (1.9.0)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from pulsar-client>=3.1.0->chromadb==0.4.12) (2025.1.31)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->transformers==4.33.0) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from requests->transformers==4.33.0) (2.3.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torch>=1.10.0->accelerate==0.22.0) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from sympy->onnxruntime>=1.14.1->chromadb==0.4.12) (1.3.0)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (8.1.8)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typer>=0.9.0->chromadb==0.4.12) (13.9.4)\n",
      "Requirement already satisfied: h11>=0.8 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn>=0.18.3->uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.14.0)\n",
      "Requirement already satisfied: httptools>=0.6.3 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.6.4)\n",
      "Requirement already satisfied: python-dotenv>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.1)\n",
      "Requirement already satisfied: uvloop!=0.15.0,!=0.15.1,>=0.14.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (0.21.0)\n",
      "Requirement already satisfied: watchfiles>=0.13 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (1.0.4)\n",
      "Requirement already satisfied: websockets>=10.4 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from uvicorn[standard]>=0.18.3->chromadb==0.4.12) (15.0.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from nltk->sentence_transformers==2.2.2) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from scikit-learn->sentence_transformers==2.2.2) (3.5.0)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from torchvision->sentence_transformers==2.2.2) (11.0.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (2.15.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain==0.0.300) (1.0.0)\n",
      "Requirement already satisfied: humanfriendly>=9.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from coloredlogs->onnxruntime>=1.14.1->chromadb==0.4.12) (10.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate==0.22.0) (2.1.5)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.9.0->chromadb==0.4.12) (0.1.2)\n",
      "Using cached transformers-4.33.0-py3-none-any.whl (7.6 MB)\n",
      "Using cached accelerate-0.22.0-py3-none-any.whl (251 kB)\n",
      "Using cached einops-0.6.1-py3-none-any.whl (42 kB)\n",
      "Using cached langchain-0.0.300-py3-none-any.whl (1.7 MB)\n",
      "Using cached bitsandbytes-0.41.1-py3-none-any.whl (92.6 MB)\n",
      "Using cached chromadb-0.4.12-py3-none-any.whl (426 kB)\n",
      "Using cached chroma_hnswlib-0.7.3-cp310-cp310-macosx_11_0_arm64.whl (197 kB)\n",
      "Using cached anyio-3.7.1-py3-none-any.whl (80 kB)\n",
      "Using cached fastapi-0.99.1-py3-none-any.whl (58 kB)\n",
      "Using cached langsmith-0.0.92-py3-none-any.whl (56 kB)\n",
      "Using cached numexpr-2.10.2-cp310-cp310-macosx_11_0_arm64.whl (134 kB)\n",
      "Using cached pulsar_client-3.6.1-cp310-cp310-macosx_13_0_universal2.whl (8.0 MB)\n",
      "Using cached pydantic-1.10.21-cp310-cp310-macosx_11_0_arm64.whl (2.6 MB)\n",
      "Using cached tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
      "Using cached tokenizers-0.13.3-cp310-cp310-macosx_12_0_arm64.whl (3.9 MB)\n",
      "Using cached starlette-0.27.0-py3-none-any.whl (66 kB)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py) ... \u001b[?25lerror\n",
      "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
      "  \n",
      "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py bdist_wheel\u001b[0m did not run successfully.\n",
      "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
      "  \u001b[31m╰─>\u001b[0m \u001b[31m[214 lines of output]\u001b[0m\n",
      "  \u001b[31m   \u001b[0m running bdist_wheel\n",
      "  \u001b[31m   \u001b[0m /opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/utils/cpp_extension.py:529: UserWarning: Attempted to use ninja as the BuildExtension backend but we could not find ninja.. Falling back to using the slow distutils backend.\n",
      "  \u001b[31m   \u001b[0m   warnings.warn(msg.format('we could not find ninja.'))\n",
      "  \u001b[31m   \u001b[0m running build\n",
      "  \u001b[31m   \u001b[0m running build_py\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/version.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/test.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_cpp_lib.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m copying xformers/info.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/fused_linear_layer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/sum_strided.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/vararg_kernel.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_sum.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_fw.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_fused_matmul_bw.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m copying xformers/triton/k_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/triton\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/simplicial_embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/residual.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/reversible.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/input_projection.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/patch_embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nvfuser.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_indexing.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_stride_sum.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_blocksparse_transformers.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_mem_eff_attn_decoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_transformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_revnet.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_swiglu.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_causal_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_fused_linear.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_encoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_triton_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_pytorch_transformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_nystrom_utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_multi_head_dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_sddmm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/benchmark_core.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/swiglu_op.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/unbind.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/common.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/indexing.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/device_limits.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/api.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/slow_ops_profiler.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m copying xformers/profiler/profiler.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/profiler\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/_csr_ops.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/blocksparse_tensor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m copying xformers/sparse/csr_tensor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/sparse\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/test_utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/hierarchical_configs.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m copying xformers/helpers/timm_sparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/helpers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/fused_softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_blocksparse_attention.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/bert_padding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton_og.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_triton.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/flash_attn_interface.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/hydra_helper.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_factory.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/model_factory.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/block_configs.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m copying xformers/factory/weight_init.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/factory\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/global_tokens.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/ortho.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/blocksparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/local.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/compositional.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/pooling.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/_sputnik_sparse.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/core.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/lambda_layer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/random.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/fourier_mix.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/scaled_dot_product.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_mask.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/linformer.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/attention_patterns.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/visual.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/sparsity_config.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/nystrom.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/favor.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mixture_of_experts.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/conv_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/fused_mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/feedforward/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/feedforward\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/vocab.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/param.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/sine.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/rotary.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/positional_embedding/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/positional_embedding\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_dropout_res_layernorm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_act_dropout.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/utils.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/nvfuser/bias_dropout_res.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/nvfuser\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/softmax.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m copying xformers/components/attention/feature_maps/base.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/components/attention/feature_maps\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_submit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/batch_fetch_results.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_with_submitit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_tasks.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/run_grid_search.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/model_wrapper.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m copying xformers/benchmarks/LRA/code/dataset.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/benchmarks/LRA/code\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/decoder.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/triton.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/dispatch.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/attn_bias.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/common.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/flash.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/small_k.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m copying xformers/ops/fmha/cutlass.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/ops/fmha\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/cross_entropy.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/losses/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/losses\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/patch_embed.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/layers/rotary.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/layers\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/pretrained.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/generation.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/benchmark.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/utils/distributed.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/utils\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gptj.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/opt.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/llama.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/vit.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/bert.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/falcon.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt_neox.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/models/gpt.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/models\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/activations.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/fused_dense.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/rms_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/ops/layer_norm.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/ops\n",
      "  \u001b[31m   \u001b[0m creating build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/embedding.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/__init__.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mlp.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/block.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m copying xformers/_flash_attn/modules/mha.py -> build/lib.macosx-11.1-arm64-cpython-310/xformers/_flash_attn/modules\n",
      "  \u001b[31m   \u001b[0m running build_ext\n",
      "  \u001b[31m   \u001b[0m building 'xformers._C' extension\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/autograd\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/cpu\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/indexing\n",
      "  \u001b[31m   \u001b[0m creating build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/swiglu\n",
      "  \u001b[31m   \u001b[0m clang++ -Wno-unused-result -Wsign-compare -Wunreachable-code -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /opt/anaconda3/envs/ai_env/include -arch arm64 -fPIC -O2 -isystem /opt/anaconda3/envs/ai_env/include -arch arm64 -I/private/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/pip-install-0iin7gad/xformers_c01172b71428445dbda77ab434b71b7b/xformers/csrc -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/torch/csrc/api/include -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/TH -I/opt/anaconda3/envs/ai_env/lib/python3.10/site-packages/torch/include/THC -I/opt/anaconda3/envs/ai_env/include/python3.10 -c xformers/csrc/attention/attention.cpp -o build/temp.macosx-11.1-arm64-cpython-310/xformers/csrc/attention/attention.o -O3 -fopenmp -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_clang\\\" -DPYBIND11_STDLIB=\\\"_libcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1002\\\" -DTORCH_EXTENSION_NAME=_C -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++17\n",
      "  \u001b[31m   \u001b[0m clang++: error: unsupported option '-fopenmp'\n",
      "  \u001b[31m   \u001b[0m error: command '/usr/bin/clang++' failed with exit code 1\n",
      "  \u001b[31m   \u001b[0m \u001b[31m[end of output]\u001b[0m\n",
      "  \n",
      "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "\u001b[31m  ERROR: Failed building wheel for xformers\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h  Running setup.py clean for xformers\n",
      "Failed to build xformers\n",
      "\u001b[31mERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# Install required libraries for LLM inference, embedding, vector storage, and orchestration\n",
    "!pip install transformers==4.33.0 accelerate==0.22.0 einops==0.6.1 \\\n",
    "langchain==0.0.300 xformers==0.0.21 bitsandbytes==0.41.1 \\\n",
    "sentence_transformers==2.2.2 chromadb==0.4.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:20.396669Z",
     "iopub.status.busy": "2025-03-20T13:29:20.396080Z",
     "iopub.status.idle": "2025-03-20T13:29:26.899824Z",
     "shell.execute_reply": "2025-03-20T13:29:26.899012Z",
     "shell.execute_reply.started": "2025-03-20T13:29:20.396640Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Core libraries\n",
    "import torch            # GPU support and tensor operations\n",
    "import os               # OS-level file handling\n",
    "import time             # Execution timing and benchmarking\n",
    "import transformers     # Hugging Face Transformers for model loading and generation\n",
    "import chromadb         # Chroma – vector database for retrieval\n",
    "import gradio as gr     # Gradio – simple web UI for LLM interaction\n",
    "\n",
    "# ChromaDB configuration\n",
    "from chromadb.config import Settings  # Configure local ChromaDB instance\n",
    "\n",
    "# LangChain modules for building a RAG pipeline\n",
    "from langchain.llms import HuggingFacePipeline                      # Wrap Hugging Face model for LangChain\n",
    "from langchain.document_loaders import TextLoader                   # Load plain text documents\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter  # Split documents into overlapping chunks\n",
    "from langchain.embeddings import HuggingFaceEmbeddings              # Create embeddings from text\n",
    "from langchain.chains import RetrievalQA                            # Combine retriever and LLM into a RAG chain\n",
    "from langchain.prompts import PromptTemplate                        # Define structured prompts for the LLM\n",
    "from langchain.vectorstores import Chroma                           # Chroma wrapper for LangChain vector store\n",
    "\n",
    "# Hugging Face model components\n",
    "from transformers import (\n",
    "    AutoConfig,               # Load model configuration\n",
    "    AutoModelForCausalLM,     # Load pre-trained causal language model\n",
    "    AutoTokenizer             # Load matching tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize model, tokenizer, query pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model, the device, and the `bitsandbytes` configuration(for cuda)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:26.901324Z",
     "iopub.status.busy": "2025-03-20T13:29:26.900871Z",
     "iopub.status.idle": "2025-03-20T13:29:26.955062Z",
     "shell.execute_reply": "2025-03-20T13:29:26.954421Z",
     "shell.execute_reply.started": "2025-03-20T13:29:26.901289Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device => mps\n"
     ]
    }
   ],
   "source": [
    "# Select the LLaMA model to be used \n",
    "hf_token = os.environ.get(\"HF_TOKEN\") \n",
    "model_id = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "\n",
    "# Configure the device (GPU, Apple Silicon, or CPU fallback)\n",
    "if torch.backends.mps.is_available():\n",
    "    device = \"mps\"  # Apple Silicon (Metal Performance Shaders)\n",
    "elif torch.cuda.is_available():\n",
    "    device = \"cuda\"  # NVIDIA GPU\n",
    "\n",
    "    # Enable 4-bit quantisation to reduce memory usage (requires bitsandbytes)\n",
    "    bnb_config = transformers.BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type='nf4',\n",
    "        bnb_4bit_use_double_quant=True,\n",
    "        bnb_4bit_compute_dtype=torch.bfloat16\n",
    "    )\n",
    "else:\n",
    "    device = \"cpu\"  # Default to CPU if no acceleration is available\n",
    "\n",
    "print(f\"Using device => {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the model and the tokenizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:29:26.957030Z",
     "iopub.status.busy": "2025-03-20T13:29:26.956689Z",
     "iopub.status.idle": "2025-03-20T13:31:44.637120Z",
     "shell.execute_reply": "2025-03-20T13:31:44.636242Z",
     "shell.execute_reply.started": "2025-03-20T13:29:26.956999Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Explain AI in one sentence: Artificial Intelligence (AI) refers to the development of computer systems that can perform tasks that would typically require human intelligence, such as learning, problem-solving, and decision-making.\n",
      "\n",
      "Explain Machine Learning (ML) in one sentence: Machine Learning (ML)\n"
     ]
    }
   ],
   "source": [
    "# Load model configuration from Hugging Face hub\n",
    "model_config = AutoConfig.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "# Load pre-trained LLaMA model with specified configuration\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    config=model_config,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "model.to(device)\n",
    "\n",
    "# Load associated tokenizer for text pre-processing\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_id,\n",
    "    trust_remote_code=True,\n",
    "    token=hf_token,\n",
    ")\n",
    "\n",
    "# Ensure pad token is defined (fallback to EOS if missing)\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Quick functional test: simple prompt inference\n",
    "prompt = \"Explain AI in one sentence:\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model.generate(**inputs, max_new_tokens=50)\n",
    "\n",
    "print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the query pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:44.638973Z",
     "iopub.status.busy": "2025-03-20T13:31:44.638336Z",
     "iopub.status.idle": "2025-03-20T13:31:46.271505Z",
     "shell.execute_reply": "2025-03-20T13:31:46.270705Z",
     "shell.execute_reply.started": "2025-03-20T13:31:44.638940Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepare pipeline: 0.051 sec.\n"
     ]
    }
   ],
   "source": [
    "# Build the text-generation pipeline using the loaded model and tokenizer\n",
    "time_1 = time.time()\n",
    "\n",
    "query_pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,  # Use reduced precision on GPU to save memory\n",
    "    device_map=\"auto\",  # Automatically selects the optimal device mapping\n",
    "    max_length=1024     # Limit output length for efficiency\n",
    ")\n",
    "\n",
    "time_2 = time.time()\n",
    "print(f\"Prepare pipeline: {round(time_2 - time_1, 3)} sec.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function for testing the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:46.272769Z",
     "iopub.status.busy": "2025-03-20T13:31:46.272516Z",
     "iopub.status.idle": "2025-03-20T13:31:46.277701Z",
     "shell.execute_reply": "2025-03-20T13:31:46.276793Z",
     "shell.execute_reply.started": "2025-03-20T13:31:46.272746Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_model(tokenizer, pipeline, prompt_to_test):\n",
    "    # Run a test query through the text-generation pipeline and print the result\n",
    "    # Arguments:\n",
    "    #   tokenizer – used to encode and decode text\n",
    "    #   pipeline – the text-generation pipeline for inference\n",
    "    #   prompt_to_test – the input prompt (string)\n",
    "\n",
    "    time_1 = time.time()\n",
    "\n",
    "    sequences = pipeline(\n",
    "        prompt_to_test,\n",
    "        do_sample=True,                  # Enable sampling for more natural variation\n",
    "        top_k=10,                        # Sample from top 10 likely tokens\n",
    "        num_return_sequences=1,         # Return a single output sequence\n",
    "        eos_token_id=tokenizer.eos_token_id,\n",
    "        max_length=200                  # Limit length of the generated response\n",
    "    )\n",
    "\n",
    "    time_2 = time.time()\n",
    "    print(f\"Test inference: {round(time_2 - time_1, 3)} sec.\")\n",
    "\n",
    "    for seq in sequences:\n",
    "        print(f\"Result: {seq['generated_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the query pipeline\n",
    "\n",
    "Test the pipeline with a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:46.281510Z",
     "iopub.status.busy": "2025-03-20T13:31:46.281274Z",
     "iopub.status.idle": "2025-03-20T13:31:51.993780Z",
     "shell.execute_reply": "2025-03-20T13:31:51.992809Z",
     "shell.execute_reply.started": "2025-03-20T13:31:46.281490Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test inference: 3.56 sec.\n",
      "Result: Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. The State of the Union address is a formal speech delivered by the President of the United States to Congress, where they discuss the state of the country's economy, national security, and other key issues. It is a critical event that marks the official beginning of the new legislative session. The speech is typically delivered in late January or early February.\n"
     ]
    }
   ],
   "source": [
    "# Run a sample prompt to validate the model's response quality and formatting\n",
    "test_model(\n",
    "    tokenizer,\n",
    "    query_pipeline,\n",
    "    \"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval Augmented Generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-09-23T19:22:16.434937Z",
     "iopub.status.busy": "2023-09-23T19:22:16.433666Z",
     "iopub.status.idle": "2023-09-23T19:22:16.440864Z",
     "shell.execute_reply": "2023-09-23T19:22:16.439217Z",
     "shell.execute_reply.started": "2023-09-23T19:22:16.434891Z"
    }
   },
   "source": [
    "## Check the model with a HuggingFace pipeline\n",
    "\n",
    "\n",
    "Check the model with a HF pipeline, using a query about the meaning of State of the Union (SOTU)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:51.995012Z",
     "iopub.status.busy": "2025-03-20T13:31:51.994748Z",
     "iopub.status.idle": "2025-03-20T13:31:56.181771Z",
     "shell.execute_reply": "2025-03-20T13:31:56.180949Z",
     "shell.execute_reply.started": "2025-03-20T13:31:51.994988Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_7883/3951405675.py:2: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_7883/3951405675.py:5: LangChainDeprecationWarning: The method `BaseLLM.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM response:\n",
      " Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words. The State of the Union address is a yearly address delivered by the President of the United States to Congress, in which the President reports on the progress of the federal government, the nation's economy, and the state of the country. It is a formal, televised event that provides an overview of the state of the nation, setting priorities and outlining legislative proposals for the upcoming year. It is an opportunity for the President to discuss domestic and foreign policy issues.\n"
     ]
    }
   ],
   "source": [
    "# Wrap the Hugging Face pipeline for use within LangChain\n",
    "llm = HuggingFacePipeline(pipeline=query_pipeline)\n",
    "\n",
    "# Quick test to verify the LLM integration with LangChain\n",
    "response = llm(prompt=\"Please explain what is the State of the Union address. Give just a definition. Keep it in 100 words.\")\n",
    "print(\"LLM response:\\n\", response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingestion of data using Text loder\n",
    "\n",
    "Ingest the presidential address, from Jan 2023."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.183114Z",
     "iopub.status.busy": "2025-03-20T13:31:56.182807Z",
     "iopub.status.idle": "2025-03-20T13:31:56.199371Z",
     "shell.execute_reply": "2025-03-20T13:31:56.198742Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.183090Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 1 documents.\n"
     ]
    }
   ],
   "source": [
    "# Load the source text file for processing\n",
    "loader = TextLoader(\"biden-sotu-2023-planned-official.txt\", encoding=\"utf8\")\n",
    "documents = loader.load()\n",
    "print(f\"Loaded {len(documents)} documents.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in chunks\n",
    "\n",
    "Split data in chunks using a recursive character text splitter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.200813Z",
     "iopub.status.busy": "2025-03-20T13:31:56.200476Z",
     "iopub.status.idle": "2025-03-20T13:31:56.217027Z",
     "shell.execute_reply": "2025-03-20T13:31:56.216199Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.200783Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total splits: 43\n"
     ]
    }
   ],
   "source": [
    "# Split the loaded document into smaller overlapping chunks for embedding\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,     # Each chunk will contain up to 1000 characters\n",
    "    chunk_overlap=20     # 20 characters of overlap between chunks to preserve context\n",
    ")\n",
    "all_splits = text_splitter.split_documents(documents)\n",
    "print(f\"Total splits: {len(all_splits)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Embeddings and Storing in Vector Store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the embeddings using Sentence Transformer and HuggingFace embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:31:56.218365Z",
     "iopub.status.busy": "2025-03-20T13:31:56.218064Z",
     "iopub.status.idle": "2025-03-20T13:32:35.967071Z",
     "shell.execute_reply": "2025-03-20T13:32:35.966381Z",
     "shell.execute_reply.started": "2025-03-20T13:31:56.218335Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_7883/3438620386.py:6: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"token\": hf_token})\n"
     ]
    }
   ],
   "source": [
    "# Set up the embedding model (using sentence-transformers: all-mpnet-base-v2)\n",
    "model_name = \"sentence-transformers/all-mpnet-base-v2\"\n",
    "model_kwargs = {\"device\": device}  # Specify target device for embedding model\n",
    "\n",
    "# Create embedding object for converting text chunks into vector representations\n",
    "embeddings = HuggingFaceEmbeddings(model_name=model_name, model_kwargs={\"token\": hf_token})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize ChromaDB with the document splits, the embeddings defined previously and with the option to persist it locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:35.968365Z",
     "iopub.status.busy": "2025-03-20T13:32:35.968113Z",
     "iopub.status.idle": "2025-03-20T13:32:38.687578Z",
     "shell.execute_reply": "2025-03-20T13:32:38.686690Z",
     "shell.execute_reply.started": "2025-03-20T13:32:35.968343Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a persistent vector database using Chroma\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=all_splits,          # Chunks of text to be embedded and stored\n",
    "    embedding=embeddings,          # Embedding model used for vectorisation\n",
    "    persist_directory=\"chroma_db\"  # Directory to store the local Chroma database\n",
    ")\n",
    "\n",
    "# Convert the vector database into a retriever for use in RAG pipelines\n",
    "retriever = vectordb.as_retriever()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a custom prompt template to guide the LLM's response behaviour\n",
    "custom_prompt = PromptTemplate(\n",
    "    input_variables=[\"context\", \"question\"],\n",
    "    template=\"\"\"You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
    "If the answer is not in the context, say \"No relevant info found.\"\n",
    "\n",
    "Return only the final answer in one to three sentences.\n",
    "Do not restate the question or context. \n",
    "Do not include these instructions in your final output.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.689063Z",
     "iopub.status.busy": "2025-03-20T13:32:38.688802Z",
     "iopub.status.idle": "2025-03-20T13:32:38.693716Z",
     "shell.execute_reply": "2025-03-20T13:32:38.692919Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.689041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a RetrievalQA chain by combining retriever, LLM, and custom prompt\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,                              # HuggingFacePipeline wrapped LLM\n",
    "    chain_type=\"stuff\",                  # Use 'stuff' chain type (basic context injection)\n",
    "    retriever=retriever,                 # Chroma-based retriever\n",
    "    verbose=False,                       # Suppress intermediate logging\n",
    "    return_source_documents=False,       # Do not return source documents with the answer\n",
    "    chain_type_kwargs={\n",
    "        \"prompt\": custom_prompt          # Inject custom prompt into the chain\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the Retrieval-Augmented Generation \n",
    "\n",
    "Define a test function that will run the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.695240Z",
     "iopub.status.busy": "2025-03-20T13:32:38.694954Z",
     "iopub.status.idle": "2025-03-20T13:32:38.707219Z",
     "shell.execute_reply": "2025-03-20T13:32:38.706136Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.695219Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def test_rag(qa, query):   \n",
    "    print(f\"Query: {query}\\n\")\n",
    "    result = qa.run(query)\n",
    "    print(\"Final Output:\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check few queries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:38.708562Z",
     "iopub.status.busy": "2025-03-20T13:32:38.708266Z",
     "iopub.status.idle": "2025-03-20T13:32:49.660554Z",
     "shell.execute_reply": "2025-03-20T13:32:49.659722Z",
     "shell.execute_reply.started": "2025-03-20T13:32:38.708536Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What were the main topics in the State of the Union in 2023? Summarise. Keep it under 200 words.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/6t/0hy_8xcj7kz5clpdw2ljwztw0000gn/T/ipykernel_7883/2215616331.py:3: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Output: You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
      "If the answer is not in the context, say \"No relevant info found.\"\n",
      "\n",
      "Return only the final answer in one to three sentences.\n",
      "Do not restate the question or context. \n",
      "Do not include these instructions in your final output.\n",
      "\n",
      "Context:\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "Question: What were the main topics in the State of the Union in 2023? Summarise. Keep it under 200 words.\n",
      "\n",
      "Answer:\n",
      "No relevant info found.\n"
     ]
    }
   ],
   "source": [
    "# Test the full RAG pipeline with a summarisation query\n",
    "query = \"What were the main topics in the State of the Union in 2023? Summarise. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:49.662268Z",
     "iopub.status.busy": "2025-03-20T13:32:49.661709Z",
     "iopub.status.idle": "2025-03-20T13:32:59.736440Z",
     "shell.execute_reply": "2025-03-20T13:32:59.735585Z",
     "shell.execute_reply.started": "2025-03-20T13:32:49.662235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "Final Output: You are a helpful AI assistant. Use only the text from the context below to answer the user's question.\n",
      "If the answer is not in the context, say \"No relevant info found.\"\n",
      "\n",
      "Return only the final answer in one to three sentences.\n",
      "Do not restate the question or context. \n",
      "Do not include these instructions in your final output.\n",
      "\n",
      "Context:\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops.\n",
      "\n",
      "Question: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "\n",
      "Answer:\n",
      "The United States is a strong nation with a stable economy. The State of the Union report highlights the nation's strong foundation, with a backbone of strong people and a soul that is truly united. The nation's economic status is one of stability, with a capacity to overcome challenges and achieve greatness. The report concludes that the nation's strength lies in its people, and that together, they can overcome any obstacle. The United States is a beacon of hope and opportunity, and its people are truly blessed. The nation\n"
     ]
    }
   ],
   "source": [
    "# Test another query\n",
    "query = \"What is the nation economic status? Summarize. Keep it under 200 words.\"\n",
    "test_rag(qa, query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document sources\n",
    "\n",
    "Check the documents sources for the last query run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_kg_hide-input": true,
    "execution": {
     "iopub.execute_input": "2025-03-20T13:32:59.737929Z",
     "iopub.status.busy": "2025-03-20T13:32:59.737646Z",
     "iopub.status.idle": "2025-03-20T13:32:59.775667Z",
     "shell.execute_reply": "2025-03-20T13:32:59.774896Z",
     "shell.execute_reply.started": "2025-03-20T13:32:59.737907Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: What is the nation economic status? Summarize. Keep it under 200 words.\n",
      "Retrieved documents: 4\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n",
      "Source:  biden-sotu-2023-planned-official.txt\n",
      "Text:  over darkness, hope over fear, unity over division. Stability over chaos. We must see each other not as enemies, but as fellow Americans. We are a good people, the only nation in the world built on an idea. That all of us, every one of us, is created equal in the image of God. A nation that stands as a beacon to the world. A nation in a new age of possibilities. So I have come here to fulfil my constitutional duty to report on the state of the union. And here is my report. Because the soul of this nation is strong, because the backbone of this nation is strong, because the people of this nation are strong, the State of the Union is strong. As I stand here tonight, I have never been more optimistic about the future of America. We just have to remember who we are. We are the United States of America and there is nothing, nothingbeyond our capacity if we do it together. May God bless you all. May God protect our troops. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manually inspect documents retrieved via similarity search\n",
    "docs = vectordb.similarity_search(query)\n",
    "\n",
    "print(f\"Query: {query}\")\n",
    "print(f\"Retrieved documents: {len(docs)}\")\n",
    "\n",
    "for doc in docs:\n",
    "    doc_details = doc.to_json()['kwargs']\n",
    "    print(\"Source: \", doc_details['metadata']['source'])     # Document source \n",
    "    print(\"Text: \", doc_details['page_content'], \"\\n\")        # Retrieved content snippet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradio Interface\n",
    "\n",
    "Create a Gradio interface to test the RAG system.\n",
    "The output will only show the answer, if the answer is not in the text, the system should respond with \"No relevant info found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://0bab91603025b08a62.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://0bab91603025b08a62.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Function to run the RAG query and extract the final answer\n",
    "def rag_qa(user_query):\n",
    "    raw_output = qa.run(user_query)\n",
    "\n",
    "    # Extract the final answer after \"Answer:\" (if present)\n",
    "    lower_text = raw_output.lower()\n",
    "    split_token = \"answer:\"\n",
    "    idx = lower_text.find(split_token)\n",
    "\n",
    "    if idx != -1:\n",
    "        # Get the text following \"Answer:\"\n",
    "        final_answer = raw_output[idx + len(split_token):].strip()\n",
    "        return final_answer\n",
    "    else:\n",
    "        # If \"Answer:\" not found, return the full output\n",
    "        return raw_output\n",
    "\n",
    "# Define interface description:\n",
    "demo_description = \"\"\"\n",
    "**Context**:\n",
    "This demo uses a Retrieval-Augmented Generation (RAG) system based on \n",
    "Biden’s 2023 State of the Union Address. \n",
    "All responses are grounded in this document. \n",
    "If no relevant information is found, the system will say \"No relevant info found.\"\n",
    "\n",
    "**Sample Questions**:\n",
    "1. What were the main topics regarding infrastructure in this speech?\n",
    "2. How does the speech address the competition with China?\n",
    "3. What does Biden say about job growth in the past two years?\n",
    "4. Does the speech mention anything about Social Security or Medicare?\n",
    "5. What does the speech propose regarding Big Tech or online privacy?\n",
    "\n",
    "Feel free to ask any question related to Biden’s 2023 State of the Union Address.\n",
    "\"\"\"\n",
    "\n",
    "# Build the Gradio interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_qa,\n",
    "    inputs=\"text\",\n",
    "    outputs=\"text\",\n",
    "    title=\"Biden 2023 SOTU RAG QA Demo\",\n",
    "    description=demo_description\n",
    ")\n",
    "\n",
    "#  Launch the app\n",
    "if __name__ == \"__main__\":\n",
    "    demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "\n",
    "\n",
    "Langchain, ChromaDB, and Llama 3.2 were used to build a Retrieval-Augmented Generation solution. For testing, the latest State of the Union address from January 2023 was used. The system was able to retrieve relevant information from the document and provide accurate answers to questions. The system can be further improved by using more data and fine-tuning the model.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2880535,
     "sourceId": 4966565,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 735,
     "modelInstanceId": 3093,
     "sourceId": 4298,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30559,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
