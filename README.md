# RAG-LLaMA: Retrieval-Augmented Generation with Llama-3.2-1B-Instruct

This project showcases **Retrieval-Augmented Generation (RAG)** using a **LLaMA 1B** model and **Chroma** as the vector database. The system answers questions about **President Biden’s 2023 State of the Union Address** by retrieving relevant text chunks, then generating a final answer.

## Key Highlights
- **Llama-3.2-1B-Instruct** Model: A smaller variant of the LLaMA family, finetuned for text-generation tasks.
- **Chroma** Vector Store: Stores text embeddings for the speech.
- **LangChain** Integration: Use RetrievalQA (chain_type="stuff") to fetch context from the vector store, then pass it to LLaMA for final generation.
- **Gradio** Demo: A simple web interface for asking questions about the State of the Union transcript.

## Project Files
- **`app.py`**  
  Main Python script. Loads the LLaMA model, sets up a pipeline, initializes the Chroma DB, and launches a Gradio interface.
- **`requirements.txt`**  
  Lists Python dependencies (Transformers, LangChain, ChromaDB, etc.).
- **`Dockerfile`**  
  Container definition. Installs dependencies, copies files, and runs `app.py`.
- **`biden-sotu-2023-planned-official.txt`**  
  The text of the 2023 State of the Union speech, used as the knowledge source.

## How It Works
1. **Model Loading**  
   Download the Llama-3.2-1B-Instruct from a Hugging Face repo (`meta-llama/Llama-3.2-1B-Instruct`), patch the config if needed, then initialize the model on device.
2. **Embedding & Retrieval**  
   Store text chunks in a local Chroma DB (`chroma_db` folder). When a query arrives, relevant chunks are retrieved via `Chroma.as_retriever()`.
3. **RAG**  
   LangChain’s `RetrievalQA` uses the retrieved chunks as context. The final answer is generated by the LLaMA pipeline.
4. **Gradio UI**  
   Serve a Gradio interface on port 7860. The user can type questions and see answers.

## Running Locally
1. **Clone** the repo and enter the directory:
   ```bash
   git clone https://github.com/YourName/rag-llama.git
   cd rag-llama
   ```
2. **Install** dependencies:
   ```bash
   pip install -r requirements.txt
   ```
4. **Run** the app:
   ```bash
   python app.py
   ```
   - By default, it launches a Gradio interface at http://127.0.0.1:7860.
## Using Docker
1. **Build** the image:
   ```bash
   docker build -t rag-llama .
   ```
2. **Run** the container:
   ```bash
   docker run -p 7860:7860 rag-llama.
   ```
   - Visit http://localhost:7860 to interact with the demo.
## Example Questions
- “What were the main topics regarding infrastructure in this speech?”
- “How does the speech address the competition with China?”
- “What does Biden say about Social Security or Medicare?”

## Key Takeaways
- **Use Case**: Demonstrates a retrieval-augmented QA system on domain-specific text (the SOTU speech).
- **Relevance**: Showcases knowledge retrieval, local embeddings, and a small language model deployment.
- **Scalability**: LLaMA 1B is CPU-friendly for demos. Larger models or GPU instances could be used for production.

## Future Improvements
- **Scaling**: Try bigger LLaMA variants or QLoRA finetuning.
- **Online RAG**: Add real-time web scraping or knowledge-base retrieval.
- **UI Enhancements**: Provide richer web interface, usage metrics, etc.

---

**Thank you for checking out RAG-LLaMA!**  
Feel free to open issues or pull requests, or connect on LinkedIn.  
